{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighbors-based regression can be used in cases where the data labels are continuous rather than discrete variables. The label assigned to a query point is computed based on the mean of the labels of its nearest neighbors.\n",
    "\n",
    "scikit-learn implements two different neighbors regressors: KNeighborsRegressor implements learning based on the  nearest neighbors of each query point, where  is an integer value specified by the user. RadiusNeighborsRegressor implements learning based on the neighbors within a fixed radius  of the query point, where  is a floating-point value specified by the user. Scikitlearn\n",
    "\n",
    "\n",
    "\n",
    "Linear regression is an example of a parametric approach because it assumes a linear functional form for f(X). Parametric methods have several advantages. They are often easy to fit, because one need estimate only a small number of coefficients. In the case of linear re- gression, the coefficients have simple interpretations, and tests of statistical significance can be easily performed. But parametric methods do have a disadvantage: by construction, they make strong assumptions about the form of f(X). If the specified functional form is far from the truth, and prediction accuracy is our goal, then the parametric method will perform poorly. For instance, if we assume a linear relationship between X and Y but the true relationship is far from linear, then the resulting model will provide a poor fit to the data, and any conclusions drawn from it will be suspect.\n",
    "\n",
    "In contrast, non-parametric methods do not explicitly assume a para- metric form for f(X), and thereby provide an alternative and more flexi- ble approach for performing regression. We discuss various non-parametric methods in this book. Here we consider one of the simplest and best-known non-parametric methods, K-nearest neighbors regression (KNN regression).  ISLR page 104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "epa = pd.read_csv('https://raw.githubusercontent.com/sqlshep/SQLShepBlog/master/data/epaMpg.csv')\n",
    "\n",
    "#Drop the row number\n",
    "epa = epa.drop(epa.columns[[0]], axis=1)\n",
    "\n",
    "#replace the \".\" in the column names with \"_\"\n",
    "epa.columns = epa.columns.str.replace('.', '_')\n",
    "\n",
    "# Drop useless columns\n",
    "epa = epa.drop(epa.columns[[0,1,2]], axis=1)\n",
    "epa = epa.drop(epa.columns[[3,9,11]], axis=1)\n",
    "\n",
    "epa['Tested_Transmission_Type_Code']= epa['Tested_Transmission_Type_Code'].astype('category')    \n",
    "epa['AxleRatio']= epa['AxleRatio'].astype('category')\n",
    "print(epa.dtypes)\n",
    "\n",
    "#One hot encode categories\n",
    "epa = pd.get_dummies(epa)\n",
    "\n",
    "\n",
    "#epa_X = epa.iloc[:, epa.columns =='Weight']\n",
    "epa_X = epa.iloc[:, epa.columns !='FuelEcon']\n",
    "epa_y = epa.iloc[:, epa.columns =='FuelEcon']\n",
    "\n",
    "# Split the training and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(epa_X, epa_y, test_size=0.20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsRegressor(n_neighbors=10)\n",
    "knn.fit(X_train, y_train)\n",
    "y_hat = knn.predict(X_test)\n",
    "#y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math \n",
    "\n",
    "# The mean squared error\n",
    "print('Mean squared error: %.2f'\n",
    "      % mean_squared_error(y_test, y_hat))\n",
    "\n",
    "# The root mean squared error\n",
    "print('Root Mean squared error: %.2f'\n",
    "      % math.sqrt(mean_squared_error(y_test, y_hat)))\n",
    "\n",
    "# The coefficient of determination: 1 is perfect prediction\n",
    "print('Coefficient of determination: %.2f'\n",
    "      % r2_score(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
