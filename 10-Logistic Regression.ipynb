{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html\n",
    "\n",
    "https://www.kaggle.com/mnassrib/titanic-logistic-regression-with-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.rc(\"font\", size=14)\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"white\") #white background style for seaborn plots\n",
    "sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV train data file into DataFrame\n",
    "train = pd.read_csv(\"https://raw.githubusercontent.com/sqlshep/SQLShepBlog/master/data/Titanic_train.csv\")\n",
    "\n",
    "# Read CSV test data file into DataFrame\n",
    "test = pd.read_csv(\"https://raw.githubusercontent.com/sqlshep/SQLShepBlog/master/data/Titainc_test.csv\")\n",
    "\n",
    "# preview train data\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in train data\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent of missing \"Age\" \n",
    "print('Percent of missing \"Age\" records is %.2f%%' %((train['Age'].isnull().sum()/train.shape[0])*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~20% of entries for passenger age are missing. Let's see what the 'Age' variable looks like in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = train[\"Age\"].hist(bins=20, density=True, stacked=True, color='teal', alpha=0.6)\n",
    "train[\"Age\"].plot(kind='density', color='teal')\n",
    "ax.set(xlabel='Age')\n",
    "plt.xlim(-10,85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since \"Age\" is (right) skewed, using the mean might give us biased results by filling in ages that are older than desired. To deal with this, we'll use the median to impute the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean age\n",
    "print('The mean of \"Age\" is %.2f' %(train[\"Age\"].mean(skipna=True)))\n",
    "# median age\n",
    "print('The median of \"Age\" is %.2f' %(train[\"Age\"].median(skipna=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent of missing \"Cabin\" \n",
    "print('Percent of missing \"Cabin\" records is %.2f%%' %((train['Cabin'].isnull().sum()/train.shape[0])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "77% of records are missing, which means that imputing information and using this variable for prediction is probably not wise. We'll ignore this variable in our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percent of missing \"Embarked\" \n",
    "print('Percent of missing \"Embarked\" records is %.2f%%' %((train['Embarked'].isnull().sum()/train.shape[0])*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 2 (0.22%) missing values for \"Embarked\", so we can just impute with the port where most people boarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Boarded passengers grouped by port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton):')\n",
    "print(train['Embarked'].value_counts())\n",
    "sns.countplot(x='Embarked', data=train, palette='Set2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The most common boarding port of embarkation is %s.' %train['Embarked'].value_counts().idxmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By far the most passengers boarded in Southhampton, so we'll impute those 2 NaN's w/ \"S\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on my assessment of the missing values in the dataset, I'll make the following changes to the data:\n",
    "\n",
    "- If \"Age\" is missing for a given row, I'll impute with 28 (median age).\n",
    "- If \"Embarked\" is missing for a riven row, I'll impute with \"S\" (the most common boarding port).\n",
    "- I'll ignore \"Cabin\" as a variable. There are too many missing values for imputation. Based on the information available, it appears that this value is associated with the passenger's class and fare paid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.copy()\n",
    "train_data[\"Age\"].fillna(train[\"Age\"].median(skipna=True), inplace=True)\n",
    "train_data[\"Embarked\"].fillna(train['Embarked'].value_counts().idxmax(), inplace=True)\n",
    "train_data.drop('Cabin', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in adjusted train data\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preview adjusted train data\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "ax = train[\"Age\"].hist(bins=15, density=True, stacked=True, color='teal', alpha=0.6)\n",
    "train[\"Age\"].plot(kind='density', color='teal')\n",
    "\n",
    "ax = train_data[\"Age\"].hist(bins=15, density=True, stacked=True, color='orange', alpha=0.5)\n",
    "train_data[\"Age\"].plot(kind='density', color='orange')\n",
    "\n",
    "ax.legend(['Raw Age', 'Adjusted Age'])\n",
    "ax.set(xlabel='Age')\n",
    "plt.xlim(-10,85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Kaggle data dictionary, both SibSp and Parch relate to traveling with family. For simplicity's sake (and to account for possible multicollinearity), I'll combine the effect of these variables into one categorical predictor: whether or not that individual was traveling alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create categorical variable for traveling alone\n",
    "train_data['TravelAlone']=np.where((train_data[\"SibSp\"]+train_data[\"Parch\"])>0, 0, 1)\n",
    "train_data.drop('SibSp', axis=1, inplace=True)\n",
    "train_data.drop('Parch', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll also create categorical variables for Passenger Class (\"Pclass\"), Gender (\"Sex\"), and Port Embarked (\"Embarked\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create categorical variables and drop some variables\n",
    "training=pd.get_dummies(train_data, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\n",
    "training.drop('Sex_female', axis=1, inplace=True)\n",
    "training.drop('PassengerId', axis=1, inplace=True)\n",
    "training.drop('Name', axis=1, inplace=True)\n",
    "training.drop('Ticket', axis=1, inplace=True)\n",
    "\n",
    "final_train = training\n",
    "final_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, apply the same changes to the test data.\n",
    "\n",
    "I will apply to same imputation for \"Age\" in the Test data as I did for my Training data (if missing, Age = 28).\n",
    "\n",
    "I'll also remove the \"Cabin\" variable from the test data, as I've decided not to include it in my analysis.\n",
    "\n",
    "There were no missing values in the \"Embarked\" port variable.\n",
    "I'll add the dummy variables to finalize the test set.\n",
    "Finally, I'll impute the 1 missing value for \"Fare\" with the median, 14.45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.copy()\n",
    "test_data[\"Age\"].fillna(train[\"Age\"].median(skipna=True), inplace=True)\n",
    "test_data[\"Fare\"].fillna(train[\"Fare\"].median(skipna=True), inplace=True)\n",
    "test_data.drop('Cabin', axis=1, inplace=True)\n",
    "\n",
    "test_data['TravelAlone']=np.where((test_data[\"SibSp\"]+test_data[\"Parch\"])>0, 0, 1)\n",
    "\n",
    "test_data.drop('SibSp', axis=1, inplace=True)\n",
    "test_data.drop('Parch', axis=1, inplace=True)\n",
    "\n",
    "testing = pd.get_dummies(test_data, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\n",
    "testing.drop('Sex_female', axis=1, inplace=True)\n",
    "testing.drop('PassengerId', axis=1, inplace=True)\n",
    "testing.drop('Name', axis=1, inplace=True)\n",
    "testing.drop('Ticket', axis=1, inplace=True)\n",
    "\n",
    "final_test = testing\n",
    "final_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "ax = sns.kdeplot(final_train[\"Age\"][final_train.Survived == 1], color=\"darkturquoise\", shade=True)\n",
    "sns.kdeplot(final_train[\"Age\"][final_train.Survived == 0], color=\"lightcoral\", shade=True)\n",
    "plt.legend(['Survived', 'Died'])\n",
    "plt.title('Density Plot of Age for Surviving Population and Deceased Population')\n",
    "ax.set(xlabel='Age')\n",
    "plt.xlim(-10,85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The age distribution for survivors and deceased is actually very similar. One notable difference is that, of the survivors, a larger proportion were children. The passengers evidently made an attempt to save children by giving them a place on the life rafts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.xticks(rotation = 90)\n",
    "avg_survival_byage = final_train[[\"Age\", \"Survived\"]].groupby(['Age'], as_index=False).mean()\n",
    "g = sns.barplot(x='Age', y='Survived', data=avg_survival_byage, color=\"LightSeaGreen\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train['IsMinor']=np.where(final_train['Age']<=16, 1, 0)\n",
    "\n",
    "final_test['IsMinor']=np.where(final_test['Age']<=16, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "ax = sns.kdeplot(final_train[\"Fare\"][final_train.Survived == 1], color=\"darkturquoise\", shade=True)\n",
    "sns.kdeplot(final_train[\"Fare\"][final_train.Survived == 0], color=\"lightcoral\", shade=True)\n",
    "plt.legend(['Survived', 'Died'])\n",
    "plt.title('Density Plot of Fare for Surviving Population and Deceased Population')\n",
    "ax.set(xlabel='Fare')\n",
    "plt.xlim(-20,200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the distributions are clearly different for the fares of survivors vs. deceased, it's likely that this would be a significant predictor in our final model. Passengers who paid lower fare appear to have been less likely to survive. This is probably strongly correlated with Passenger Class, which we'll look at next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot('Pclass', 'Survived', data=train, color=\"darkturquoise\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, being a first class passenger was safest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot('Embarked', 'Survived', data=train, color=\"teal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passengers who boarded in Cherbourg, France, appear to have the highest survival rate. Passengers who boarded in Southhampton were marginally less likely to survive than those who boarded in Queenstown. This is probably related to passenger class, or maybe even the order of room assignments (e.g. maybe earlier passengers were more likely to have rooms closer to deck).\n",
    "It's also worth noting the size of the whiskers in these plots. Because the number of passengers who boarded at Southhampton was highest, the confidence around the survival rate is the highest. The whisker of the Queenstown plot includes the Southhampton average, as well as the lower bound of its whisker. It's possible that Queenstown passengers were equally, or even more, ill-fated than their Southhampton counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot('TravelAlone', 'Survived', data=final_train, color=\"mediumturquoise\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individuals traveling without family were more likely to die in the disaster than those with family aboard. Given the era, it's likely that individuals traveling alone were likely male."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot('Sex', 'Survived', data=train, color=\"aquamarine\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very obvious difference. Clearly being female greatly increased your chances of survival."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an external estimator that assigns weights to features, recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a coef_ attribute or through a feature_importances_ attribute. Then, the least important features are pruned from current set of features.That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.\n",
    "\n",
    "\n",
    "References:\n",
    "http://scikit-learn.org/stable/modules/feature_selection.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "cols = [\"Age\",\"Fare\",\"TravelAlone\",\"Pclass_1\",\"Pclass_2\",\"Embarked_C\",\"Embarked_S\",\"Sex_male\",\"IsMinor\"] \n",
    "X = final_train[cols]\n",
    "y = final_train['Survived']\n",
    "\n",
    "# Build a logreg and compute the feature importances\n",
    "model = LogisticRegression()\n",
    "\n",
    "# create the RFE model and select 8 attributes\n",
    "rfe = RFE(model, 8)\n",
    "rfe = rfe.fit(X, y)\n",
    "\n",
    "# summarize the selection of the attributes\n",
    "print('Selected features: %s' % list(X.columns[rfe.support_]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "# The \"accuracy\" scoring is proportional to the number of correct classifications\n",
    "rfecv = RFECV(estimator=LogisticRegression(), step=1, cv=10, scoring='accuracy')\n",
    "rfecv.fit(X, y)\n",
    "\n",
    "print(\"Optimal number of features: %d\" % rfecv.n_features_)\n",
    "print('Selected features: %s' % list(X.columns[rfecv.support_]))\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Selected_features = ['Age', 'TravelAlone', 'Pclass_1', 'Pclass_2', 'Embarked_C', \n",
    "                     'Embarked_S', 'Sex_male', 'IsMinor']\n",
    "X = final_train[Selected_features]\n",
    "\n",
    "plt.subplots(figsize=(8, 5))\n",
    "sns.heatmap(X.corr(), annot=True, cmap=\"RdYlGn\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n",
    "\n",
    "# create X (features) and y (response)\n",
    "X = final_train[Selected_features]\n",
    "y = final_train['Survived']\n",
    "\n",
    "# use train/test split with different random_state values\n",
    "# we can change the random_state values that changes the accuracy scores\n",
    "# the scores change a lot, this is why testing scores is a high-variance estimate\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "\n",
    "# check classification scores of logistic regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_pred_proba = logreg.predict_proba(X_test)[:, 1]\n",
    "[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\n",
    "print('Train/Test split results:')\n",
    "print(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\n",
    "print(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba))\n",
    "print(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=5, random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = clf.predict_proba(X_test)[:, 1]\n",
    "[fpr, tpr, thr] = roc_curve(y_test, preds)\n",
    "print('Train/Test split results:')\n",
    "print(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, preds))\n",
    "print(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba))\n",
    "print(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
